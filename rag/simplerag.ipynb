{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a5620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4615ae5",
   "metadata": {},
   "source": [
    "# LangChain Imports and Olloma Connection\n",
    "This notebook demonstrates how to import LangChain modules and connect to the Olloma LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f45009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LangChain is an open-source platform designed to help developers build, test, and deploy decentralized applications (dApps) on the Ethereum blockchain. It provides a simple and intuitive interface for creating and managing smart contracts, as well as tools for debugging and testing dApps before deployment.\n",
      "\n",
      "LangChain aims to make it easier for developers to work with smart contracts by providing a range of features, including:\n",
      "\n",
      "1. Simple syntax: LangChain uses a simple and intuitive syntax that makes it easy for developers to write and manage smart contracts.\n",
      "2. Drag-and-drop interface: LangChain provides a drag-and-drop interface that allows developers to easily create and edit smart contracts without having to write code.\n",
      "3. Automatic compilation: LangChain automatically compiles smart contracts into Ethereum bytecode, making it easier for developers to deploy their dApps on the blockchain.\n",
      "4. Debugging tools: LangChain provides a range of debugging tools that allow developers to test and debug their smart contracts before deployment.\n",
      "5. Deployment: LangChain simplifies the process of deploying smart contracts on the Ethereum blockchain, making it easier for developers to get their dApps up and running.\n",
      "\n",
      "Overall, LangChain is designed to make it easier for developers to build and deploy decentralized applications on the Ethereum blockchain, without requiring extensive knowledge of smart contract development or blockchain technology.\n"
     ]
    }
   ],
   "source": [
    "# Import updated LangChain Ollama module\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Function to create an LLM for a given model name\n",
    "def get_llm(model_name=\"llama2\"):\n",
    "    return OllamaLLM(model=model_name)\n",
    "\n",
    "# Connect to Ollama LLM\n",
    "llm = get_llm(\"llama2\")  # You can change to another model, e.g., \"mistral\", \"phi\", etc.\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\"\n",
    ")\n",
    "\n",
    "# Use the new RunnableSequence approach\n",
    "chain = prompt | llm\n",
    "\n",
    "# Example usage: pass a question to the chain\n",
    "response = chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a951a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, RAG appears to be an abbreviation of \"Retrieval-Augmented Generation.\" This can refer to a technique or approach that combines retrieval and generation to improve the accuracy of answers. The context documents mention RAG in relation to LangChain and Ollama, which are frameworks for developing applications powered by language models.\n",
      "Loaded 10 Wikipedia docs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.document_loaders import WikipediaLoader, PyPDFLoader\n",
    "\n",
    "# Function to create an LLM for a given model name\n",
    "def get_llm(model_name=\"llama2\"):\n",
    "    return OllamaLLM(model=model_name)\n",
    "\n",
    "# Connect to Ollama LLM\n",
    "llm = get_llm(\"llama2\")  # You can change to another model, e.g., \"mistral\", \"phi\", etc.\n",
    "\n",
    "# Example documents for retrieval\n",
    "docs = [\n",
    "    \"LangChain is a framework for developing applications powered by language models.\",\n",
    "    \"Ollama provides a local server for running open-source LLMs like Llama 2.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) combines retrieval and generation for more accurate answers.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "split_docs = text_splitter.create_documents(docs)\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OllamaEmbeddings(model=\"llama2\")\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "# RAG chain: retrieve relevant docs, then generate answer\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Use the following context to answer the question.\\nContext: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage: pass a question to the RAG chain\n",
    "response = rag_chain.invoke(\"What is RAG?\")\n",
    "print(response)\n",
    "\n",
    "# Function to load Wikipedia documents\n",
    "def load_wikipedia_docs(query, lang=\"en\", max_docs=10):\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=max_docs)\n",
    "    return loader.load()\n",
    "\n",
    "# Function to load PDF documents\n",
    "def load_pdf_docs(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    return loader.load()\n",
    "\n",
    "# Example: Load Wikipedia docs\n",
    "wiki_docs = load_wikipedia_docs(\"LangChain\")\n",
    "print(f\"Loaded {len(wiki_docs)} Wikipedia docs.\")\n",
    "\n",
    "# Example: Load PDF docs (replace 'example.pdf' with your PDF file path)\n",
    "# pdf_docs = load_pdf_docs(\"example.pdf\")\n",
    "# print(f\"Loaded {len(pdf_docs)} PDF docs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
